@INPROCEEDINGS{UberBinarySize,
  author={Chabbi, Milind and Lin, Jin and Barik, Raj},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={An Experience with Code-Size Optimization for Production iOS Mobile Applications}, 
  year={2021},
  volume={},
  number={},
  pages={363-377},
  keywords={Pipelines;Production;Software;Libraries;Mobile applications;High level languages;Optimization;code-size;machine outlining;iOS;swift;intermodule optimization;whole-program optimization},
  doi={10.1109/CGO51591.2021.9370306}
}

@inproceedings{HyFM:FunctionMergingForFree,
author = {Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Hazelwood, Kim and Leather, Hugh},
title = {HyFM: function merging for free},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463852},
doi = {10.1145/3461648.3463852},
abstract = {Function merging is an important optimization for reducing code size. It merges multiple functions into a single one, eliminating duplicate code among them. The existing state-of-the-art relies on a well-known sequence alignment algorithm to identify duplicate code across whole functions. However, this algorithm is quadratic in time and space on the number of instructions. This leads to very high time overheads and prohibitive levels of memory usage even for medium-sized benchmarks. For larger programs, it becomes impractical. This is made worse by an overly eager merging approach. All selected pairs of functions will be merged. Only then will this approach estimate the potential benefit from merging and decide whether to replace the original functions with the merged one. Given that most pairs are unprofitable, a significant amount of time is wasted producing merged functions that are simply thrown away. In this paper, we propose HyFM, a novel function merging technique that delivers similar levels of code size reduction for significantly lower time overhead and memory usage. Unlike the state-of-the-art, our alignment strategy works at the block level. Since basic blocks are usually much shorter than functions, even a quadratic alignment is acceptable. However, we also propose a linear algorithm for aligning blocks of the same size at a much lower cost. We extend this strategy with a multi-tier profitability analysis that bails out early from unprofitable merging attempts. By aligning individual pairs of blocks, we are able to decide their alignment’s profitability separately and before actually generating code. Experimental results on SPEC 2006 and 2017 show that HyFM needs orders of magnitude less memory, using up to 48 MB or 5.6 MB, depending on the variant used, while the state-of-the-art requires 32 GB in the worst case. HyFM also runs over 4.5\texttimes{}\texttimes{} faster, while still achieving comparable code size reduction. Combined with the speedup of later compilation stages due to the reduced number of functions, HyFM contributes to a reduced end-to-end compilation time.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {110–121},
numpages = {12},
keywords = {Link-Time Optimization, LLVM, Interprocedural Optimization, Function Merging, Code-Size Reduction},
location = {Virtual, Canada},
series = {LCTES 2021}
}

@inproceedings{SalSSA,
author = {Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Leather, Hugh},
title = {Effective function merging in the SSA form},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386030},
doi = {10.1145/3385412.3386030},
abstract = {Function merging is an important optimization for reducing code size. This technique eliminates redundant code across functions by merging them into a single function. While initially limited to identical or trivially similar functions, the most recent approach can identify all merging opportunities in arbitrary pairs of functions. However, this approach has a serious limitation which prevents it from reaching its full potential. Because it cannot handle phi-nodes, the state-of-the-art applies register demotion to eliminate them before applying its core algorithm. While a superficially minor workaround, this has a three-fold negative effect: by artificially lengthening the instruction sequences to be aligned, it hinders the identification of mergeable instruction; it prevents a vast number of functions from being profitably merged; it increases compilation overheads, both in terms of compile-time and memory usage.  We present SalSSA, a novel approach that fully supports the SSA form, removing any need for register demotion. By doing so, we notably increase the number of profitably merged functions. We implement SalSSA in LLVM and apply it to the SPEC 2006 and 2017 suites. Experimental results show that our approach delivers on average, 7.9\% to 9.7\% reduction on the final size of the compiled code. This translates to around 2x more code size reduction over the state-of-the-art. Moreover, as a result of aligning shorter sequences of instructions and reducing the number of wasteful merge operations, our new approach incurs an average compile-time overhead of only 5\%, 3x less than the state-of-the-art, while also reducing memory usage by over 2x.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {854–868},
numpages = {15},
keywords = {Code Size Reduction, Function Merging, LTO},
location = {London, UK},
series = {PLDI 2020}
}


@INPROCEEDINGS{F3M:FastFocusedFunctionMerging,
  author={Stirling, Sean and Rodrigo C. O., Rocha and Hazelwood, Kim and Leather, Hugh and O’Boyle, Michael and Petoumenos, Pavlos},
  booktitle={2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={F3M: Fast Focused Function Merging}, 
  year={2022},
  volume={},
  number={},
  pages={242-253},
  keywords={Measurement;Codes;Correlation;Profitability;Merging;Termination of employment;Focusing;Code-Size Reduction;Function Merging;LLVM;Compiler Optimization},
  doi={10.1109/CGO53902.2022.9741269}
}


@MISC{LLVMMainPage,
  title        = "The {LLVM} Compiler Infrastructure Project",
  booktitle    = "Llvm.org",
  howpublished = "\url{https://llvm.org/}",
  note         = "Accessed: 2025-4-23"
}

@MISC{LLVMIR,
  title        = "{LLVM} Language Reference Manual --- {LLVM} 21.0.0git
                  documentation",
  booktitle    = "Llvm.org",
  howpublished = "\url{https://llvm.org/docs/LangRef.html}",
  note         = "Accessed: 2025-4-23",
  language     = "en"
}

@MISC{LLVMPasses,
  title        = "{LLVM's} Analysis and Transform Passes --- {LLVM} 21.0.0git
                  documentation",
  booktitle    = "Llvm.org",
  howpublished = "\url{https://llvm.org/docs/Passes.html}",
  note         = "Accessed: 2025-4-23",
  language     = "en"
}

@misc{LLVMMergeFunctionsPass,
	author = {},
	title = {{M}erge{F}unctions pass, how it works \&#x2014; {L}{L}{V}{M} 21.0.0git documentation --- llvm.org},
	howpublished = {\url{https://llvm.org/docs/MergeFunctions.html}},
	year = {},
	note = {[Accessed 18-03-2025]},
}

@MISC{LLVMFuncMergSrc,
  title        = "{LLVM}: {Lib/transforms/IPO/MergeFunctions.Cpp} source file",
  booktitle    = "Llvm.org",
  howpublished = "\url{https://llvm.org/doxygen/MergeFunctions_8cpp_source.html}",
  note         = "Accessed: 2025-3-17",
  language     = "en"
}

@misc{LLVMBasicBlockDef,
	author = {},
	title = {{L}{L}{V}{M}: llvm::{B}asic{B}lock {C}lass {R}eference --- llvm.org},
	howpublished = {\url{https://llvm.org/doxygen/classllvm_1_1BasicBlock.html}},
	year = {},
	note = {[Accessed 18-03-2025]},
}

@INPROCEEDINGS{FunctionMergingSequenceAlignment,
  author={Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Leather, Hugh},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Function Merging by Sequence Alignment}, 
  year={2019},
  volume={},
  number={},
  pages={149-163},
  keywords={Merging;Production;Optimization;Benchmark testing;Bioinformatics;Performance evaluation;C++ languages;Code Size;Function Merging;IPO;LTO},
  doi={10.1109/CGO.2019.8661174}
}


@article{NeedlemanWunschAlgorithm,
title = {A general method applicable to the search for similarities in the amino acid sequence of two proteins},
journal = {Journal of Molecular Biology},
volume = {48},
number = {3},
pages = {443-453},
year = {1970},
issn = {0022-2836},
doi = {https://doi.org/10.1016/0022-2836(70)90057-4},
url = {https://www.sciencedirect.com/science/article/pii/0022283670900574},
author = {Saul B. Needleman and Christian D. Wunsch},
abstract = {A computer adaptable method for finding similarities in the amino acid sequences of two proteins has been developed. From these findings it is possible to determine whether significant homology exists between the proteins. This information is used to trace their possible evolutionary development. The maximum match is a number dependent upon the similarity of the sequences. One of its definitions is the largest number of amino acids of one protein that can be matched with those of a second protein allowing for all possible interruptions in either of the sequences. While the interruptions give rise to a very large number of comparisons, the method efficiently excludes from consideration those comparisons that cannot contribute to the maximum match. Comparisons are made from the smallest unit of significance, a pair of amino acids, one from each protein. All possible pairs are represented by a two-dimensional array, and all possible comparisons are represented by pathways through the array. For this maximum match only certain of the possible pathways must be evaluated. A numerical value, one in this case, is assigned to every cell in the array representing like amino acids. The maximum match is the largest number that would result from summing the cell values of every pathway.}
}

@INPROCEEDINGS{F3M:MinHash,
  author={Broder, A.Z.},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)}, 
  title={On the resemblance and containment of documents}, 
  year={1997},
  volume={},
  number={},
  pages={21-29},
  keywords={Sampling methods;Web sites;Digital systems;Particle measurements;Fingerprint recognition;Explosions;Algorithm design and analysis;Clustering algorithms;Costs;Testing},
  doi={10.1109/SEQUEN.1997.666900}}

@inproceedings{F3M:LSH,
author = {Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S.},
title = {Locality-sensitive hashing scheme based on p-stable distributions},
year = {2004},
isbn = {1581138857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/997817.997857},
doi = {10.1145/997817.997857},
abstract = {We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p<1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain "bounded growth" condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.},
booktitle = {Proceedings of the Twentieth Annual Symposium on Computational Geometry},
pages = {253–262},
numpages = {10},
keywords = {sublinear algorithm, locally sensitive hashing, approximate nearest neighbor, p-stable distributions},
location = {Brooklyn, New York, USA},
series = {SCG '04}
}

@article{ControlFlowGraph,
title = {The rise of machine learning for detection and classification of malware: Research developments, trends and challenges},
journal = {Journal of Network and Computer Applications},
volume = {153},
pages = {102526},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102526},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303868},
author = {Daniel Gibert and Carles Mateu and Jordi Planes},
keywords = {Malware detection, Feature engineering, Machine learning, Deep learning, Multimodal learning},
abstract = {The struggle between security analysts and malware developers is a never-ending battle with the complexity of malware changing as quickly as innovation grows. Current state-of-the-art research focus on the development and application of machine learning techniques for malware detection due to its ability to keep pace with malware evolution. This survey aims at providing a systematic and detailed overview of machine learning techniques for malware detection and in particular, deep learning techniques. The main contributions of the paper are: (1) it provides a complete description of the methods and features in a traditional machine learning workflow for malware detection and classification, (2) it explores the challenges and limitations of traditional machine learning and (3) it analyzes recent trends and developments in the field with special emphasis on deep learning approaches. Furthermore, (4) it presents the research issues and unsolved challenges of the state-of-the-art techniques and (5) it discusses the new directions of research. The survey helps researchers to have an understanding of the malware detection field and of the new developments and directions of research explored by the scientific community to tackle the problem.}
}

@article{BayesianOptimisation,
title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {26-40},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@article{MLParadigms,
author = {Shyam, Radhey and Chakraborty, Riya},
year = {2021},
month = {09},
pages = {2021},
title = {Machine Learning and Its Dominant Paradigms},
volume = {8},
doi = {10.37591/JoARB}
}

@inproceedings{BayesTheorem,
  title={Bayes's theorem},
  author={Bayes, Thomas and Hume, David},
  booktitle={Proceedings ofthe British Academy},
  volume={113},
  pages={91--109},
  year={1763}
}

@INPROCEEDINGS{BayesianFasterThanGrid,
  author={Alibrahim, Hussain and Ludwig, Simone A.},
  booktitle={2021 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={Hyperparameter Optimization: Comparing Genetic Algorithm against Grid Search and Bayesian Optimization}, 
  year={2021},
  volume={},
  number={},
  pages={1551-1559},
  keywords={Training;Machine learning algorithms;Neural networks;Prediction algorithms;Search problems;Time measurement;Bayes methods;Hyperparmeter optimization;Grid Search;Bayesian;Genetic Algorithm},
  doi={10.1109/CEC45853.2021.9504761}}


@INPROCEEDINGS{ImbalancedDataset,
  author={Pereira, Jeanne and Saraiva, Filipe},
  booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Comparative Analysis of Unbalanced Data Handling Techniques for Machine Learning Algorithms to Electricity Theft Detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  keywords={Machine learning;Support vector machines;Measurement;Radio frequency;Data handling;Training;Feature extraction;Classification problem;Electricity theft;Unbalanced data;Machine Learning},
  doi={10.1109/CEC48606.2020.9185822}}


@Inbook{LearningDefinition1,
author="Lampropoulos, Aristomenis S.
and Tsihrintzis, George A.",
title="The Learning Problem",
bookTitle="Machine Learning Paradigms: Applications in Recommender Systems",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="31--61",
abstract="In general, Learning can be defined as the modification of a behavior tendency according to experiences which have been acquired. Thus, Learning embeds distinctive attributes of intelligent behavior. Machine Learning is the study of how to develop algorithms, computer applications, and systems that have the ability to learn and, thus, improve through experience their performance at some tasks. This chapter presents the formalization of the Machine Learning Problem.",
isbn="978-3-319-19135-5",
doi="10.1007/978-3-319-19135-5_3",
url="https://doi.org/10.1007/978-3-319-19135-5_3"
}

@book{LearningDefinition2,
  title={Machine learning for audio, image and video analysis: theory and applications},
  author={Camastra, Francesco and Vinciarelli, Alessandro},
  year={2015},
  publisher={Springer}
}



@article{SupervisedLearningAndTrainingProcess,
  title = {Machine Learning from Theory to Algorithms: An Overview},
  volume = {1142},
  ISSN = {1742-6596},
  url = {http://dx.doi.org/10.1088/1742-6596/1142/1/012012},
  DOI = {10.1088/1742-6596/1142/1/012012},
  journal = {Journal of Physics: Conference Series},
  publisher = {IOP Publishing},
  author = {Alzubi,  Jafar and Nayyar,  Anand and Kumar,  Akshi},
  year = {2018},
  month = nov,
  pages = {012012}
}

@book{DeepLearningGoodfellow,
  title={Deep learning},
  author={Bengio, Yoshua and Goodfellow, Ian and Courville, Aaron and others},
  volume={1},
  year={2017},
  publisher={MIT press Cambridge, MA, USA}
}

@misc{GradientDescentAlgorithms,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1609.04747}, 
}

@article{GradientDescentBackpropagation1,
  title = {Learning representations by back-propagating errors},
  volume = {323},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/323533a0},
  DOI = {10.1038/323533a0},
  number = {6088},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Rumelhart,  David E. and Hinton,  Geoffrey E. and Williams,  Ronald J.},
  year = {1986},
  month = oct,
  pages = {533–536}
}

@incollection{LearningRate,
  title={Gradient Descent},
  author={Bishop, Christopher M and Bishop, Hugh},
  booktitle={Deep Learning: Foundations and Concepts},
  pages={209--232},
  year={2023},
  publisher={Springer}
}

@article{BatchSizeTooLarge,
title = {The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset},
journal = {ICT Express},
volume = {6},
number = {4},
pages = {312-315},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405959519303455},
author = {Ibrahem Kandel and Mauro Castelli},
keywords = {Convolutional neural networks, Deep learning, Image classification, Medical images, Batch size},
abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.}
}

@misc{BatchSizeVariance,
title={The Impact of the Mini-batch Size on the Dynamics of {\{}SGD{\}}: Variance and Beyond},
author={Xin Qian and Diego Klabjan},
year={2021},
url={https://openreview.net/forum?id=53WS781RzT9}
}

@article{BayesOptimisationOriginal,
title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimizationb},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {26-40},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@article{GradientDescentBackPropagation2,
    author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
    title = {Backpropagation Applied to Handwritten Zip Code Recognition},
    journal = {Neural Computation},
    volume = {1},
    number = {4},
    pages = {541-551},
    year = {1989},
    month = {12},
    abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
    issn = {0899-7667},
    doi = {10.1162/neco.1989.1.4.541},
    url = {https://doi.org/10.1162/neco.1989.1.4.541},
    eprint = {https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf},
}





@misc{Word2Vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@inproceedings{GloVe,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}

@inproceedings{TranslationalEncoding,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Dur\'{a}n, Alberto and Weston, Jason and Yakhnenko, Oksana},
title = {Translating embeddings for modeling multi-relational data},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2787–2795},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@article{IR2Vec,
author = {VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
title = {IR2VEC: LLVM IR Based Scalable Program Embeddings},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418463},
doi = {10.1145/3418463},
abstract = {We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information.We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {32},
numpages = {27},
keywords = {LLVM, compiler optimizations, heterogeneous systems, intermediate representations, representation learning}
}

@MISC{AppleBuildSize,
  title        = "Maximum build file sizes",
  booktitle    = "Apple.com",
  author       = "{Apple Inc}",
  howpublished = "\url{https://developer.apple.com/help/app-store-connect/reference/maximum-build-file-sizes/}",
  note         = "Accessed: 2025-4-7",
  language     = "en"
}

@MISC{GoogleBuildSize,
  title        = "Create and set up your app",
  booktitle    = "Google.com",
  howpublished = "\url{https://support.google.com/googleplay/android-developer/answer/9859152?hl=en}",
  note         = "Accessed: 2025-4-7",
  language     = "en"
}

@article{MooresLaw,
  title = {Establishing Moore’s Law},
  volume = {28},
  ISSN = {1058-6180},
  url = {http://dx.doi.org/10.1109/MAHC.2006.45},
  DOI = {10.1109/mahc.2006.45},
  number = {3},
  journal = {IEEE Annals of the History of Computing},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Mollick,  E.},
  year = {2006},
  month = jul,
  pages = {62–75}
}

@INPROCEEDINGS{EmbeddedSystemMemoryArea,
  author={Mühlbauer, Felix and Schröder, Lukas and Skoncej, Patryk and Schölzel, Mario},
  booktitle={2017 18th IEEE Latin American Test Symposium (LATS)}, 
  title={Handling manufacturing and aging faults with software-based techniques in tiny embedded systems}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Manufacturing;Redundancy;Memory management;Aging;Embedded systems},
  doi={10.1109/LATW.2017.7906756}}

@article{EmbeddedSystemMemoryCost,
author = {Yang, Lei and Dick, Robert P. and Lekatsas, Haris and Chakradhar, Srimat},
title = {Online memory compression for embedded systems},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/1698772.1698785},
doi = {10.1145/1698772.1698785},
abstract = {Memory is a scarce resource during embedded system design. Increasing memory often increases packaging costs, cooling costs, size, and power consumption. This article presents CRAMES, a novel and efficient software-based RAM compression technique for embedded systems. The goal of CRAMES is to dramatically increase effective memory capacity without hardware or application design changes, while maintaining high performance and low energy consumption. To achieve this goal, CRAMES takes advantage of an operating system's virtual memory infrastructure by storing swapped-out pages in compressed format. It dynamically adjusts the size of the compressed RAM area, protecting applications capable of running without it from performance or energy consumption penalties. In addition to compressing working data sets, CRAMES also enables efficient in-RAM filesystem compression, thereby further increasing RAM capacity.CRAMES was implemented as a loadable module for the Linux kernel and evaluated on a battery-powered embedded system. Experimental results indicate that CRAMES is capable of doubling the amount of RAM available to applications running on the original system hardware. Execution time and energy consumption for a broad range of examples are rarely affected. When physical RAM is reduced to 62.5\% of its original quantity, CRAMES enables the target embedded system to support the same applications with reasonable performance and energy consumption penalties (on average 9.5\% and 10.5\%), while without CRAMES those applications either may not execute or suffer from extreme performance degradation or instability. In addition to presenting a novel framework for dynamic data memory compression and in-RAM filesystem compression in embedded systems, this work identifies the software-based compression algorithms that are most appropriate for use in low-power embedded systems.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = mar,
articleno = {27},
numpages = {30},
keywords = {memory, compression, Embedded system}
}

@inproceedings{10.1145/2597809.2597811,
author = {Edler von Koch, Tobias J.K. and Franke, Bj\"{o}rn and Bhandarkar, Pranav and Dasgupta, Anshuman},
title = {Exploiting function similarity for code size reduction},
year = {2014},
isbn = {9781450328777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597809.2597811},
doi = {10.1145/2597809.2597811},
abstract = {For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4\%, and up to 11.5\% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.},
booktitle = {Proceedings of the 2014 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems},
pages = {85–94},
numpages = {10},
keywords = {function similarity, function merging, code size},
location = {Edinburgh, United Kingdom},
series = {LCTES '14}
}

@article{SIgmoidRELUGraph,
author = {Zhang, Long and Liu, Yangyuan and Zhou, Jianmin and Luo, Muxu and Pu, Shengxin and Yang, Xiaotong},
year = {2022},
month = {11},
pages = {8749},
title = {An Imbalanced Fault Diagnosis Method Based on TFFO and CNN for Rotating Machinery},
volume = {22},
journal = {Sensors},
doi = {10.3390/s22228749}
}

@article{FunctionMergingIsomorphicCFG,
author = {Edler von Koch, Tobias J.K. and Franke, Bj\"{o}rn and Bhandarkar, Pranav and Dasgupta, Anshuman},
title = {Exploiting function similarity for code size reduction},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666357.2597811},
doi = {10.1145/2666357.2597811},
abstract = {For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4\%, and up to 11.5\% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.},
journal = {SIGPLAN Not.},
month = jun,
pages = {85–94},
numpages = {10},
keywords = {function similarity, function merging, code size}
}

@article{CPPTemplateCodeDuplication,
  title={Safe ICF: Pointer safe and unwinding aware identical code folding in the gold linker},
  author={Tallam, Sriraman and Coutant, Cary and Taylor, Ian L and Li, David X and Demetriou, Chris},
  journal={Proceedings of the 2010 GCC Summit},
  pages={107--114},
  year={2010},
  publisher={Citeseer}
}

@misc{AttentionComplexity,
      title={On The Computational Complexity of Self-Attention}, 
      author={Feyza Duman Keles and Pruthuvi Mahesakya Wijewardena and Chinmay Hegde},
      year={2022},
      eprint={2209.04881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.04881}, 
}

@misc{AttentionIsAllYouNeed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{SelfAttentionEmbedding,
      title={A Structured Self-attentive Sentence Embedding}, 
      author={Zhouhan Lin and Minwei Feng and Cicero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
      year={2017},
      eprint={1703.03130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1703.03130}, 
}

@article{DropoutPaper,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {regularization, neural networks, model combination, deep learning}
}

@MISC{TensorflowDropout,
  title        = "{Tf.Keras.Layers.Dropout}",
  booktitle    = "{TensorFlow}",
  howpublished = "\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout}",
  note         = "Accessed: 2025-4-8",
  language     = "en"
}

@MISC{PyTorchDropout,
  title        = "Dropout --- {PyTorch} 2.6 documentation",
  booktitle    = "Pytorch.org",
  howpublished = "\url{https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html}",
  note         = "Accessed: 2025-4-8",
  language     = "en"
}

@Inbook{SiameseModelIntro,
author="Chicco, Davide",
editor="Cartwright, Hugh",
title="Siamese Neural Networks: An Overview",
bookTitle="Artificial Neural Networks",
year="2021",
publisher="Springer US",
address="New York, NY",
pages="73--94",
abstract="Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman's rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.",
isbn="978-1-0716-0826-5",
doi="10.1007/978-1-0716-0826-5_3",
url="https://doi.org/10.1007/978-1-0716-0826-5_3"
}

@inproceedings{SiameseModelSignatureVerification,
 author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S\"{a}ckinger, Eduard and Shah, Roopak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Signature Verification using a "Siamese" Time Delay Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},
 volume = {6},
 year = {1993}
}

@misc{SiameseFacialRecognition,
      title={Deep Learning Based Face Recognition Method using Siamese Network}, 
      author={Enoch Solomon and Abraham Woubie and Eyael Solomon Emiru},
      year={2024},
      eprint={2312.14001},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14001}, 
}


@Article{SiameseImageSimilarity,
AUTHOR = {Livieris, Ioannis E. and Pintelas, Emmanuel and Kiriakidou, Niki and Pintelas, Panagiotis},
TITLE = {Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM},
JOURNAL = {Journal of Imaging},
VOLUME = {9},
YEAR = {2023},
NUMBER = {10},
ARTICLE-NUMBER = {224},
URL = {https://www.mdpi.com/2313-433X/9/10/224},
PubMedID = {37888331},
ISSN = {2313-433X},
ABSTRACT = {With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustworthiness and user acceptance of image-based systems in real-world image similarity applications.},
DOI = {10.3390/jimaging9100224}
}


@article{OptimisationSelectionML,
author = {Fursin, Grigori and Miranda, Cupertino and Temam, Olivier and Namolaru, Mircea and Zaks, Ayal and Mendelson, Bilha and Bonilla, Edwin and Thomson, John and Leather, Hugh and Williams, Chris and O'Boyle, Michael and Barnard, Phil and Ashton, Elton and Courtois, Eric and Bodin, François},
year = {2008},
month = {06},
pages = {},
title = {MILEPOST GCC: machine learning based research compiler},
journal = {Proceedings of the GCC Developers' Summit 2008}
}

@INPROCEEDINGS{IterativeCompilationWActiveLearningML,
  author={Ogilvie, William F. and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
  booktitle={2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Minimizing the cost of iterative compilation with active learning}, 
  year={2017},
  volume={},
  number={},
  pages={245-256},
  keywords={Optimization;Runtime;Training;Noise measurement;Tuning;Layout;Hardware;Active Learning;Compilers;Iterative Compilation;Machine Learning;Sequential Analysis},
  doi={10.1109/CGO.2017.7863744}}


@misc{LoopVectorisationML,
      title={NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning}, 
      author={Ameer Haj-Ali and Nesreen K. Ahmed and Ted Willke and Sophia Shao and Krste Asanovic and Ion Stoica},
      year={2020},
      eprint={1909.13639},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1909.13639}, 
}

@inproceedings{FunctionInliningML,
   title={Work-in-Progress: MLGOPerf: An ML Guided Inliner to Optimize Performance},
   url={http://dx.doi.org/10.1109/CASES55004.2022.00008},
   DOI={10.1109/cases55004.2022.00008},
   booktitle={2022 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems (CASES)},
   publisher={IEEE},
   author={Ashouri, Amir H. and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing},
   year={2022},
   month=oct, pages={3–4} }


@misc{GPUCompilerML,
      title={Generating GPU Compiler Heuristics using Reinforcement Learning}, 
      author={Ian Colbert and Jake Daly and Norm Rubin},
      year={2021},
      eprint={2111.12055},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.12055}, 
}

@inproceedings{RegisterAllocationRL,
   title={RL4ReAl: Reinforcement Learning for Register Allocation},
   url={http://dx.doi.org/10.1145/3578360.3580273},
   DOI={10.1145/3578360.3580273},
   booktitle={Proceedings of the 32nd ACM SIGPLAN International Conference on Compiler Construction},
   publisher={ACM},
   author={VenkataKeerthy, S. and Jain, Siddharth and Kundu, Anilava and Aggarwal, Rohit and Cohen, Albert and Upadrasta, Ramakrishna},
   year={2023},
   month=feb, pages={133–144},
   collection={CC ’23} }

@INPROCEEDINGS{LoopUnrollingML,
  author={Stephenson, M. and Amarasinghe, S.},
  booktitle={International Symposium on Code Generation and Optimization}, 
  title={Predicting unroll factors using supervised classification}, 
  year={2005},
  volume={},
  number={},
  pages={123-134},
  keywords={Pipeline processing;Machine learning;Humans;Optimizing compilers;Support vector machines;Support vector machine classification;History;Computer science;Artificial intelligence;Laboratories},
  doi={10.1109/CGO.2005.29}}
