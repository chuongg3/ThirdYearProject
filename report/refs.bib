@INPROCEEDINGS{UberBinarySize,
  author={Chabbi, Milind and Lin, Jin and Barik, Raj},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={An Experience with Code-Size Optimization for Production iOS Mobile Applications}, 
  year={2021},
  volume={},
  number={},
  pages={363-377},
  keywords={Pipelines;Production;Software;Libraries;Mobile applications;High level languages;Optimization;code-size;machine outlining;iOS;swift;intermodule optimization;whole-program optimization},
  doi={10.1109/CGO51591.2021.9370306}
}

@inproceedings{HyFM:FunctionMergingForFree,
author = {Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Hazelwood, Kim and Leather, Hugh},
title = {HyFM: function merging for free},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463852},
doi = {10.1145/3461648.3463852},
abstract = {Function merging is an important optimization for reducing code size. It merges multiple functions into a single one, eliminating duplicate code among them. The existing state-of-the-art relies on a well-known sequence alignment algorithm to identify duplicate code across whole functions. However, this algorithm is quadratic in time and space on the number of instructions. This leads to very high time overheads and prohibitive levels of memory usage even for medium-sized benchmarks. For larger programs, it becomes impractical. This is made worse by an overly eager merging approach. All selected pairs of functions will be merged. Only then will this approach estimate the potential benefit from merging and decide whether to replace the original functions with the merged one. Given that most pairs are unprofitable, a significant amount of time is wasted producing merged functions that are simply thrown away. In this paper, we propose HyFM, a novel function merging technique that delivers similar levels of code size reduction for significantly lower time overhead and memory usage. Unlike the state-of-the-art, our alignment strategy works at the block level. Since basic blocks are usually much shorter than functions, even a quadratic alignment is acceptable. However, we also propose a linear algorithm for aligning blocks of the same size at a much lower cost. We extend this strategy with a multi-tier profitability analysis that bails out early from unprofitable merging attempts. By aligning individual pairs of blocks, we are able to decide their alignment’s profitability separately and before actually generating code. Experimental results on SPEC 2006 and 2017 show that HyFM needs orders of magnitude less memory, using up to 48 MB or 5.6 MB, depending on the variant used, while the state-of-the-art requires 32 GB in the worst case. HyFM also runs over 4.5\texttimes{}\texttimes{} faster, while still achieving comparable code size reduction. Combined with the speedup of later compilation stages due to the reduced number of functions, HyFM contributes to a reduced end-to-end compilation time.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {110–121},
numpages = {12},
keywords = {Link-Time Optimization, LLVM, Interprocedural Optimization, Function Merging, Code-Size Reduction},
location = {Virtual, Canada},
series = {LCTES 2021}
}


@INPROCEEDINGS{F3M:FastFocusedFunctionMerging,
  author={Stirling, Sean and Rodrigo C. O., Rocha and Hazelwood, Kim and Leather, Hugh and O’Boyle, Michael and Petoumenos, Pavlos},
  booktitle={2022 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={F3M: Fast Focused Function Merging}, 
  year={2022},
  volume={},
  number={},
  pages={242-253},
  keywords={Measurement;Codes;Correlation;Profitability;Merging;Termination of employment;Focusing;Code-Size Reduction;Function Merging;LLVM;Compiler Optimization},
  doi={10.1109/CGO53902.2022.9741269}
}

@misc{LLVMMergeFunctionsPass,
	author = {},
	title = {{M}erge{F}unctions pass, how it works \&#x2014; {L}{L}{V}{M} 21.0.0git documentation --- llvm.org},
	howpublished = {\url{https://llvm.org/docs/MergeFunctions.html}},
	year = {},
	note = {[Accessed 18-03-2025]},
}

@MISC{LLVMFuncMergSrc,
  title        = "{LLVM}: {Lib/transforms/IPO/MergeFunctions.Cpp} source file",
  booktitle    = "Llvm.org",
  howpublished = "\url{https://llvm.org/doxygen/MergeFunctions_8cpp_source.html}",
  note         = "Accessed: 2025-3-17",
  language     = "en"
}

@misc{LLVMBasicBlockDef,
	author = {},
	title = {{L}{L}{V}{M}: llvm::{B}asic{B}lock {C}lass {R}eference --- llvm.org},
	howpublished = {\url{https://llvm.org/doxygen/classllvm_1_1BasicBlock.html}},
	year = {},
	note = {[Accessed 18-03-2025]},
}

@INPROCEEDINGS{FunctionMergingSequenceAlignment,
  author={Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Leather, Hugh},
  booktitle={2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Function Merging by Sequence Alignment}, 
  year={2019},
  volume={},
  number={},
  pages={149-163},
  keywords={Merging;Production;Optimization;Benchmark testing;Bioinformatics;Performance evaluation;C++ languages;Code Size;Function Merging;IPO;LTO},
  doi={10.1109/CGO.2019.8661174}
}


@article{NeedlemanWunschAlgorithm,
title = {A general method applicable to the search for similarities in the amino acid sequence of two proteins},
journal = {Journal of Molecular Biology},
volume = {48},
number = {3},
pages = {443-453},
year = {1970},
issn = {0022-2836},
doi = {https://doi.org/10.1016/0022-2836(70)90057-4},
url = {https://www.sciencedirect.com/science/article/pii/0022283670900574},
author = {Saul B. Needleman and Christian D. Wunsch},
abstract = {A computer adaptable method for finding similarities in the amino acid sequences of two proteins has been developed. From these findings it is possible to determine whether significant homology exists between the proteins. This information is used to trace their possible evolutionary development. The maximum match is a number dependent upon the similarity of the sequences. One of its definitions is the largest number of amino acids of one protein that can be matched with those of a second protein allowing for all possible interruptions in either of the sequences. While the interruptions give rise to a very large number of comparisons, the method efficiently excludes from consideration those comparisons that cannot contribute to the maximum match. Comparisons are made from the smallest unit of significance, a pair of amino acids, one from each protein. All possible pairs are represented by a two-dimensional array, and all possible comparisons are represented by pathways through the array. For this maximum match only certain of the possible pathways must be evaluated. A numerical value, one in this case, is assigned to every cell in the array representing like amino acids. The maximum match is the largest number that would result from summing the cell values of every pathway.}
}

@article{BayesianOptimisation,
title = {Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization},
journal = {Journal of Electronic Science and Technology},
volume = {17},
number = {1},
pages = {26-40},
year = {2019},
issn = {1674-862X},
doi = {https://doi.org/10.11989/JEST.1674-862X.80904120},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X19300047},
author = {Jia Wu and Xiu-Yun Chen and Hao Zhang and Li-Dong Xiong and Hang Lei and Si-Hao Deng},
keywords = {Bayesian optimization, Gaussian process, hyperparameter optimization, machine learning},
abstract = {Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.}
}

@article{MLParadigms,
author = {Shyam, Radhey and Chakraborty, Riya},
year = {2021},
month = {09},
pages = {2021},
title = {Machine Learning and Its Dominant Paradigms},
volume = {8},
doi = {10.37591/JoARB}
}

@INPROCEEDINGS{ImbalancedDataset,
  author={Pereira, Jeanne and Saraiva, Filipe},
  booktitle={2020 IEEE Congress on Evolutionary Computation (CEC)}, 
  title={A Comparative Analysis of Unbalanced Data Handling Techniques for Machine Learning Algorithms to Electricity Theft Detection}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  keywords={Machine learning;Support vector machines;Measurement;Radio frequency;Data handling;Training;Feature extraction;Classification problem;Electricity theft;Unbalanced data;Machine Learning},
  doi={10.1109/CEC48606.2020.9185822}}


@Inbook{LearningDefinition1,
author="Lampropoulos, Aristomenis S.
and Tsihrintzis, George A.",
title="The Learning Problem",
bookTitle="Machine Learning Paradigms: Applications in Recommender Systems",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="31--61",
abstract="In general, Learning can be defined as the modification of a behavior tendency according to experiences which have been acquired. Thus, Learning embeds distinctive attributes of intelligent behavior. Machine Learning is the study of how to develop algorithms, computer applications, and systems that have the ability to learn and, thus, improve through experience their performance at some tasks. This chapter presents the formalization of the Machine Learning Problem.",
isbn="978-3-319-19135-5",
doi="10.1007/978-3-319-19135-5_3",
url="https://doi.org/10.1007/978-3-319-19135-5_3"
}

@book{LearningDefinition2,
  title={Machine learning for audio, image and video analysis: theory and applications},
  author={Camastra, Francesco and Vinciarelli, Alessandro},
  year={2015},
  publisher={Springer}
}



@article{SupervisedLearningAndTrainingProcess,
  title = {Machine Learning from Theory to Algorithms: An Overview},
  volume = {1142},
  ISSN = {1742-6596},
  url = {http://dx.doi.org/10.1088/1742-6596/1142/1/012012},
  DOI = {10.1088/1742-6596/1142/1/012012},
  journal = {Journal of Physics: Conference Series},
  publisher = {IOP Publishing},
  author = {Alzubi,  Jafar and Nayyar,  Anand and Kumar,  Akshi},
  year = {2018},
  month = nov,
  pages = {012012}
}

@book{DeepLearningGoodfellow,
  title={Deep learning},
  author={Bengio, Yoshua and Goodfellow, Ian and Courville, Aaron and others},
  volume={1},
  year={2017},
  publisher={MIT press Cambridge, MA, USA}
}


@misc{Word2Vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}

@inproceedings{GloVe,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}

@inproceedings{TranslationalEncoding,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Dur\'{a}n, Alberto and Weston, Jason and Yakhnenko, Oksana},
title = {Translating embeddings for modeling multi-relational data},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2787–2795},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@article{IR2Vec,
author = {VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
title = {IR2VEC: LLVM IR Based Scalable Program Embeddings},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3418463},
doi = {10.1145/3418463},
abstract = {We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information.We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.},
journal = {ACM Trans. Archit. Code Optim.},
month = dec,
articleno = {32},
numpages = {27},
keywords = {LLVM, compiler optimizations, heterogeneous systems, intermediate representations, representation learning}
}

@MISC{AppleBuildSize,
  title        = "Maximum build file sizes",
  booktitle    = "Apple.com",
  author       = "{Apple Inc}",
  howpublished = "\url{https://developer.apple.com/help/app-store-connect/reference/maximum-build-file-sizes/}",
  note         = "Accessed: 2025-4-7",
  language     = "en"
}

@MISC{GoogleBuildSize,
  title        = "Create and set up your app",
  booktitle    = "Google.com",
  howpublished = "\url{https://support.google.com/googleplay/android-developer/answer/9859152?hl=en}",
  note         = "Accessed: 2025-4-7",
  language     = "en"
}

@article{MooresLaw,
  title = {Establishing Moore’s Law},
  volume = {28},
  ISSN = {1058-6180},
  url = {http://dx.doi.org/10.1109/MAHC.2006.45},
  DOI = {10.1109/mahc.2006.45},
  number = {3},
  journal = {IEEE Annals of the History of Computing},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Mollick,  E.},
  year = {2006},
  month = jul,
  pages = {62–75}
}

@INPROCEEDINGS{EmbeddedSystemMemoryArea,
  author={Mühlbauer, Felix and Schröder, Lukas and Skoncej, Patryk and Schölzel, Mario},
  booktitle={2017 18th IEEE Latin American Test Symposium (LATS)}, 
  title={Handling manufacturing and aging faults with software-based techniques in tiny embedded systems}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Manufacturing;Redundancy;Memory management;Aging;Embedded systems},
  doi={10.1109/LATW.2017.7906756}}

@article{EmbeddedSystemMemoryCost,
author = {Yang, Lei and Dick, Robert P. and Lekatsas, Haris and Chakradhar, Srimat},
title = {Online memory compression for embedded systems},
year = {2010},
issue_date = {February 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/1698772.1698785},
doi = {10.1145/1698772.1698785},
abstract = {Memory is a scarce resource during embedded system design. Increasing memory often increases packaging costs, cooling costs, size, and power consumption. This article presents CRAMES, a novel and efficient software-based RAM compression technique for embedded systems. The goal of CRAMES is to dramatically increase effective memory capacity without hardware or application design changes, while maintaining high performance and low energy consumption. To achieve this goal, CRAMES takes advantage of an operating system's virtual memory infrastructure by storing swapped-out pages in compressed format. It dynamically adjusts the size of the compressed RAM area, protecting applications capable of running without it from performance or energy consumption penalties. In addition to compressing working data sets, CRAMES also enables efficient in-RAM filesystem compression, thereby further increasing RAM capacity.CRAMES was implemented as a loadable module for the Linux kernel and evaluated on a battery-powered embedded system. Experimental results indicate that CRAMES is capable of doubling the amount of RAM available to applications running on the original system hardware. Execution time and energy consumption for a broad range of examples are rarely affected. When physical RAM is reduced to 62.5\% of its original quantity, CRAMES enables the target embedded system to support the same applications with reasonable performance and energy consumption penalties (on average 9.5\% and 10.5\%), while without CRAMES those applications either may not execute or suffer from extreme performance degradation or instability. In addition to presenting a novel framework for dynamic data memory compression and in-RAM filesystem compression in embedded systems, this work identifies the software-based compression algorithms that are most appropriate for use in low-power embedded systems.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = mar,
articleno = {27},
numpages = {30},
keywords = {memory, compression, Embedded system}
}

@inproceedings{10.1145/2597809.2597811,
author = {Edler von Koch, Tobias J.K. and Franke, Bj\"{o}rn and Bhandarkar, Pranav and Dasgupta, Anshuman},
title = {Exploiting function similarity for code size reduction},
year = {2014},
isbn = {9781450328777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597809.2597811},
doi = {10.1145/2597809.2597811},
abstract = {For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4\%, and up to 11.5\% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.},
booktitle = {Proceedings of the 2014 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems},
pages = {85–94},
numpages = {10},
keywords = {function similarity, function merging, code size},
location = {Edinburgh, United Kingdom},
series = {LCTES '14}
}

@article{FunctionMergingIsomorphicCFG,
author = {Edler von Koch, Tobias J.K. and Franke, Bj\"{o}rn and Bhandarkar, Pranav and Dasgupta, Anshuman},
title = {Exploiting function similarity for code size reduction},
year = {2014},
issue_date = {May 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666357.2597811},
doi = {10.1145/2666357.2597811},
abstract = {For cost-sensitive or memory constrained embedded systems, code size is at least as important as performance. Consequently, compact code generation has become a major focus of attention within the compiler community. In this paper we develop a pragmatic, yet effective code size reduction technique, which exploits structural similarity of functions. It avoids code duplication through merging of similar functions and targeted insertion of control flow to resolve small differences. We have implemented our purely software based and platform-independent technique in the LLVM compiler frame work and evaluated it against the SPEC CPU2006 benchmarks and three target platforms: Intel x86, ARM based Qualcomm Krait(TM), and Qualcomm Hexagon(TM) DSP. We demonstrate that code size for SPEC CPU2006 can be reduced by more than 550KB on x86. This corresponds to an overall code size reduction of 4\%, and up to 11.5\% for individual programs. Overhead introduced by additional control flow is compensated for by better I-cache performance of the compacted programs. We also show that identifying suitable candidates and subsequent merging of functions can be implemented efficiently.},
journal = {SIGPLAN Not.},
month = jun,
pages = {85–94},
numpages = {10},
keywords = {function similarity, function merging, code size}
}

@article{CPPTemplateCodeDuplication,
  title={Safe ICF: Pointer safe and unwinding aware identical code folding in the gold linker},
  author={Tallam, Sriraman and Coutant, Cary and Taylor, Ian L and Li, David X and Demetriou, Chris},
  journal={Proceedings of the 2010 GCC Summit},
  pages={107--114},
  year={2010},
  publisher={Citeseer}
}

@misc{AttentionComplexity,
      title={On The Computational Complexity of Self-Attention}, 
      author={Feyza Duman Keles and Pruthuvi Mahesakya Wijewardena and Chinmay Hegde},
      year={2022},
      eprint={2209.04881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.04881}, 
}

@misc{AttentionIsAllYouNeed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{SelfAttentionEmbedding,
      title={A Structured Self-attentive Sentence Embedding}, 
      author={Zhouhan Lin and Minwei Feng and Cicero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
      year={2017},
      eprint={1703.03130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1703.03130}, 
}

@article{DropoutPaper,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: a simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1929–1958},
numpages = {30},
keywords = {regularization, neural networks, model combination, deep learning}
}

@MISC{TensorflowDropout,
  title        = "{Tf.Keras.Layers.Dropout}",
  booktitle    = "{TensorFlow}",
  howpublished = "\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout}",
  note         = "Accessed: 2025-4-8",
  language     = "en"
}

@MISC{PyTorchDropout,
  title        = "Dropout --- {PyTorch} 2.6 documentation",
  booktitle    = "Pytorch.org",
  howpublished = "\url{https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html}",
  note         = "Accessed: 2025-4-8",
  language     = "en"
}

@Inbook{SiameseModelIntro,
author="Chicco, Davide",
editor="Cartwright, Hugh",
title="Siamese Neural Networks: An Overview",
bookTitle="Artificial Neural Networks",
year="2021",
publisher="Springer US",
address="New York, NY",
pages="73--94",
abstract="Similarity has always been a key aspect in computer science and statistics. Any time two element vectors are compared, many different similarity approaches can be used, depending on the final goal of the comparison (Euclidean distance, Pearson correlation coefficient, Spearman's rank correlation coefficient, and others). But if the comparison has to be applied to more complex data samples, with features having different dimensionality and types which might need compression before processing, these measures would be unsuitable. In these cases, a siamese neural network may be the best choice: it consists of two identical artificial neural networks each capable of learning the hidden representation of an input vector. The two neural networks are both feedforward perceptrons, and employ error back-propagation during training; they work parallelly in tandem and compare their outputs at the end, usually through a cosine distance. The output generated by a siamese neural network execution can be considered the semantic similarity between the projected representation of the two input vectors. In this overview we first describe the siamese neural network architecture, and then we outline its main applications in a number of computational fields since its appearance in 1994. Additionally, we list the programming languages, software packages, tutorials, and guides that can be practically used by readers to implement this powerful machine learning model.",
isbn="978-1-0716-0826-5",
doi="10.1007/978-1-0716-0826-5_3",
url="https://doi.org/10.1007/978-1-0716-0826-5_3"
}

@inproceedings{SiameseModelSignatureVerification,
 author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S\"{a}ckinger, Eduard and Shah, Roopak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Cowan and G. Tesauro and J. Alspector},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Signature Verification using a "Siamese" Time Delay Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/1993/file/288cc0ff022877bd3df94bc9360b9c5d-Paper.pdf},
 volume = {6},
 year = {1993}
}

@misc{SiameseFacialRecognition,
      title={Deep Learning Based Face Recognition Method using Siamese Network}, 
      author={Enoch Solomon and Abraham Woubie and Eyael Solomon Emiru},
      year={2024},
      eprint={2312.14001},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14001}, 
}


@Article{SiameseImageSimilarity,
AUTHOR = {Livieris, Ioannis E. and Pintelas, Emmanuel and Kiriakidou, Niki and Pintelas, Panagiotis},
TITLE = {Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM},
JOURNAL = {Journal of Imaging},
VOLUME = {9},
YEAR = {2023},
NUMBER = {10},
ARTICLE-NUMBER = {224},
URL = {https://www.mdpi.com/2313-433X/9/10/224},
PubMedID = {37888331},
ISSN = {2313-433X},
ABSTRACT = {With the proliferation of image-based applications in various domains, the need for accurate and interpretable image similarity measures has become increasingly critical. Existing image similarity models often lack transparency, making it challenging to understand the reasons why two images are considered similar. In this paper, we propose the concept of explainable image similarity, where the goal is the development of an approach, which is capable of providing similarity scores along with visual factual and counterfactual explanations. Along this line, we present a new framework, which integrates Siamese Networks and Grad-CAM for providing explainable image similarity and discuss the potential benefits and challenges of adopting this approach. In addition, we provide a comprehensive discussion about factual and counterfactual explanations provided by the proposed framework for assisting decision making. The proposed approach has the potential to enhance the interpretability, trustworthiness and user acceptance of image-based systems in real-world image similarity applications.},
DOI = {10.3390/jimaging9100224}
}


@article{OptimisationSelectionML,
author = {Fursin, Grigori and Miranda, Cupertino and Temam, Olivier and Namolaru, Mircea and Zaks, Ayal and Mendelson, Bilha and Bonilla, Edwin and Thomson, John and Leather, Hugh and Williams, Chris and O'Boyle, Michael and Barnard, Phil and Ashton, Elton and Courtois, Eric and Bodin, François},
year = {2008},
month = {06},
pages = {},
title = {MILEPOST GCC: machine learning based research compiler},
journal = {Proceedings of the GCC Developers' Summit 2008}
}

@INPROCEEDINGS{IterativeCompilationWActiveLearningML,
  author={Ogilvie, William F. and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
  booktitle={2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={Minimizing the cost of iterative compilation with active learning}, 
  year={2017},
  volume={},
  number={},
  pages={245-256},
  keywords={Optimization;Runtime;Training;Noise measurement;Tuning;Layout;Hardware;Active Learning;Compilers;Iterative Compilation;Machine Learning;Sequential Analysis},
  doi={10.1109/CGO.2017.7863744}}


@misc{LoopVectorisationML,
      title={NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning}, 
      author={Ameer Haj-Ali and Nesreen K. Ahmed and Ted Willke and Sophia Shao and Krste Asanovic and Ion Stoica},
      year={2020},
      eprint={1909.13639},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1909.13639}, 
}

@inproceedings{FunctionInliningML,
   title={Work-in-Progress: MLGOPerf: An ML Guided Inliner to Optimize Performance},
   url={http://dx.doi.org/10.1109/CASES55004.2022.00008},
   DOI={10.1109/cases55004.2022.00008},
   booktitle={2022 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems (CASES)},
   publisher={IEEE},
   author={Ashouri, Amir H. and Elhoushi, Mostafa and Hua, Yuzhe and Wang, Xiang and Manzoor, Muhammad Asif and Chan, Bryan and Gao, Yaoqing},
   year={2022},
   month=oct, pages={3–4} }
